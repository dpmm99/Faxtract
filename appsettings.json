{
  "Logging": {
    "LogLevel": {
      "Default": "Information",
      "Microsoft.AspNetCore": "Warning"
    }
  },
  "AllowedHosts": "*",
  "LLamaConfig": {
    "ModelPath": "C:\\AI\\phi-4-Q4_K_M.gguf",
    //Tested models:
    //phi-4-Q4_K_M.gguf is fairly good.
    //Qwen3-14B-UD-Q5_K_XL.gguf is very good.
    //DeepSeek-R1-0528-Qwen3-8B-Q6_K.gguf is questionable.
    //Mistral-Small-3.2-24B-Instruct-2506-UD-Q5_K_XL is also good, but you shouldn't quantize the KV cache; its quality greatly suffers as context grows when KV is quantized.
    //I tried Gemma 3 27B (the 4-bit QAT one), Qwen3 30B-A3B Q6_K, and Qwen3 4B Q6_K, but they're all far worse at following the instructions than Phi-4, and only Qwen3-4B is anywhere near as fast.
    //Mistral-Small-24B-Instruct-2501-Q5_K_M.gguf is also single-digit tokens/second with a big batch.
    //Also tried Qwen3-32B-UD-Q2_K_XL.gguf but it was super slow despite being quite small because it used shared sysmem; turning that off made it fast.
    "BatchSize": 4096,
    //"TensorBufferOverrides": "blk\\.(2[6-9]|[3-4][0-9]).*=CPU", //What I'd use for Qwen3-30B-A3B-Q6_K, but it's an MoE--less benefit from batching--and it's a thinking model, but I didn't implement anything to look for and cut out <think> blocks.
    "TensorSplits": "0,100",
    "UseTemplateFromGguf": true,
    //"TypeK": "GGML_TYPE_Q8_0", //See Llama.Native.GGMLType, but the options are pretty much F16 for both or Q8_0 for both.
    //"TypeV": "GGML_TYPE_Q8_0",
    "Threads": 6,
    "GpuLayerCount": -1,
    "PrePromptFile": "preprompt.state",
    "PrePromptText": "Generate an appropriate number of high-quality flash cards from the given text. Use clear, concise, unambiguous language. Vary question types (definition, application, relationship, process). Ensure questions require no external context other than the topic, and cards must not refer to each other or to 'this text'. If the text is complex, generate more flash cards than you would if it were simple. Notate the question and answer strictly with \"Q:\" and \"A:\", each at the start of a line, no bold, italics, numbering, # headers, etc. Do not write an intro or conclusion. If the given text doesn't appear educational in nature, just reject the request. The text:\n",
    "ExtraContextPrefix": "(Topic or other context: ",
    "ExtraContextSuffix": ")\n",
    "Temperature": 0.6,
    "TopK": 40,
    "TopP": 0.9,
    "MinP": 0.1,
    "MaxTokens": 2048, //1024 for non-thinking models is usually about enough.
    "WorkBatchSize": 12,
    "MinimumWorkBatchSize": 8,
    "MaxBatchWaitTimeSeconds": 90,
    "AllowContinuousBatching": true //May not work well because of a LlamaSharp or llama.cpp bug causing the conversations to not free up much of their context when disposed. It doesn't seem like LlamaSharp is passing the wrong numbers to the KV cache removal function, though.
  }
}
